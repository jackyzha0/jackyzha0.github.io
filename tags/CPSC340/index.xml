<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CPSC340 on jzhao.xyz</title>
    <link>https://jzhao.xyz/tags/CPSC340/</link>
    <description>Recent content in CPSC340 on jzhao.xyz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://jzhao.xyz/tags/CPSC340/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fundamental Tradeoff</title>
      <link>https://jzhao.xyz/thoughts/fundamental-tradeoff/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/fundamental-tradeoff/</guid>
      <description>The fundamental tradeoff has two parts:
 How small you can make the training error How well training error approximates the test error.</description>
    </item>
    
    <item>
      <title>Autoencoders</title>
      <link>https://jzhao.xyz/thoughts/Autoencoders/</link>
      <pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Autoencoders/</guid>
      <description>Autoencoders are neural networks with same input and output. They are [[thoughts/latent-factor model|latent-factor models]]
Architecture:
 Includes a bottleneck layer: with dimension $k$ smaller than input $d$.</description>
    </item>
    
    <item>
      <title>Generative Models</title>
      <link>https://jzhao.xyz/thoughts/generative-models/</link>
      <pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/generative-models/</guid>
      <description>Given data, we want to make more data that look like it
Last 10 years have seen a variety of new deep generative models:</description>
    </item>
    
    <item>
      <title>LSTM</title>
      <link>https://jzhao.xyz/thoughts/LSTM/</link>
      <pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/LSTM/</guid>
      <description>Long short term memory (LSTM) models are variant of RNNs. They are modified to try to remember short-term $z$ and long-term dependencies $c$.</description>
    </item>
    
    <item>
      <title>Automatic Differentiation (AD)</title>
      <link>https://jzhao.xyz/thoughts/automatic-differentiation/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/automatic-differentiation/</guid>
      <description>Input: code computing a function Output: code to compute one or more derivatives of the function.  AD writes functions as a sequence of simple compositions $$f_5(f_4(f_3(f_2(f_1(x)))))$$</description>
    </item>
    
    <item>
      <title>Latent-Factor Models</title>
      <link>https://jzhao.xyz/thoughts/latent-factor-model/</link>
      <pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/latent-factor-model/</guid>
      <description>Like [[thoughts/change of basis]] but instead of hand-picking the features, we learn them from data.
Part weights are a change of basis from $x_i$ to some $z_i$.</description>
    </item>
    
    <item>
      <title>Maximum a Posteriori (MAP) Estimation</title>
      <link>https://jzhao.xyz/thoughts/maximum-a-posteriori-estimation/</link>
      <pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/maximum-a-posteriori-estimation/</guid>
      <description>Maximizes $\hat w \in \arg\max_w { P(w|D) }$
Given our data, what is the model $w$ is the best model?</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation (MLE)</title>
      <link>https://jzhao.xyz/thoughts/maximum-likelihood-estimation/</link>
      <pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/maximum-likelihood-estimation/</guid>
      <description>Maximizes $\hat w \in \arg \max_w { P(D|w) }$
Suppose we have a dataset $D$ with parameters $w$. For example,</description>
    </item>
    
    <item>
      <title>XGBoost</title>
      <link>https://jzhao.xyz/thoughts/XGBoost/</link>
      <pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/XGBoost/</guid>
      <description>Uses [[thoughts/regularization|regularized]] regression trees. These are like [[thoughts/decision tree|decision trees]] where each split is
 based on a single feature each leaf gives a real-valued prediction  Fitting a regression tree  Train: set each weight $w_L$ at leaf $L$ by minimizing squared error $\sum_{i=1}^n (w_{L_i}-y_i)^2$  We use using greedy recursive splitting for growing the tree   Prediction: At each leaf, the prediction $\hat y_i$ is the mean of all $y_i$ that fall under that leaf node  Ensemble and Boosting We create a series of trees that are trained on the residual of the previous tree.</description>
    </item>
    
    <item>
      <title>Change of basis</title>
      <link>https://jzhao.xyz/thoughts/change-of-basis/</link>
      <pubDate>Thu, 03 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/change-of-basis/</guid>
      <description>Effectively by constructing new features that take the variable to certain powers. To get a y-intercept (bias), we just raise $x$ to the 0th power to get 1.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://jzhao.xyz/thoughts/convolutional-neural-networks/</link>
      <pubDate>Thu, 03 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/convolutional-neural-networks/</guid>
      <description>Rather than picking from fixed convolutions, we learn the elements of the filters. A convolution is a [[thoughts/imaging#Linear Filters|linear filter]] that measures the effect one signal has on another signal.</description>
    </item>
    
    <item>
      <title>Binary Classification</title>
      <link>https://jzhao.xyz/thoughts/binary-classification/</link>
      <pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/binary-classification/</guid>
      <description>Set $y_i = +1$ for one class (“important”) Set $y_i = -1$ for the other class (“not important”) To predict, we look at whether $w^Tx_i$ is closer to +1 or -1  $\hat y_i = \textrm{sign} (w^Tx_i)$    Least squares error may overpenalize.</description>
    </item>
    
    <item>
      <title>Multi-class Classification</title>
      <link>https://jzhao.xyz/thoughts/multi-class-classification/</link>
      <pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/multi-class-classification/</guid>
      <description>One vs All Suppose we only know how to do [[thoughts/probabilistic classifier|probabilistic binary classification]]. But we have $k$ classes we want to distinguish between.</description>
    </item>
    
    <item>
      <title>Probabilistic Classifier</title>
      <link>https://jzhao.xyz/thoughts/probabilistic-classifier/</link>
      <pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/probabilistic-classifier/</guid>
      <description>We want a model of $P(y_i = \textrm{important} | x_i )$ for use in [[thoughts/Decision theory|decision theory]].
 Predictions generally map $w^Tx_i$ to labels for classes (for binary prediction, we used $\textrm{sign}(x)$) Probabilities we want to map $w^Tx_i$ to the range $[0,1]$  The most common choice is to use the sigmoid function:</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://jzhao.xyz/thoughts/SVM/</link>
      <pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/SVM/</guid>
      <description>An SVM is just Hinge loss with L2-regularization
$$f(w) = \sum_{i=1}^n \max{0, 1-y_iw^Tx_i}$$
They can also be viewed as &amp;lsquo;maximizing the margin&amp;rsquo;:</description>
    </item>
    
    <item>
      <title>Gaussian RBFs</title>
      <link>https://jzhao.xyz/thoughts/Gaussian-RBF/</link>
      <pubDate>Fri, 21 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Gaussian-RBF/</guid>
      <description>Non-parametric basis. Can think about it as a sum of gaussian &amp;lsquo;bumps&amp;rsquo;.
Replace $x_i = (x_{i1}, x_{i2}, \dots, x_{in})$ with</description>
    </item>
    
    <item>
      <title>Hyper-parameter Optimization</title>
      <link>https://jzhao.xyz/thoughts/hyper-parameter-optimization/</link>
      <pubDate>Fri, 21 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/hyper-parameter-optimization/</guid>
      <description>How do we efficiently find the “best” hyper-parameters?
More complicated models have even more hyper-parameters. This makes searching all values expensive (increases over-fitting risk)</description>
    </item>
    
    <item>
      <title>Regularization</title>
      <link>https://jzhao.xyz/thoughts/regularization/</link>
      <pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/regularization/</guid>
      <description>A method for controlling [[thoughts/complexity|complexity]]. Our main tools:
 Model averaging (e.g. [[thoughts/Ensemble method|ensemble methods]]) Regularization (this)  When we have multiple models with the same training models, we should pick models that are more conservative (e.</description>
    </item>
    
    <item>
      <title>Feature Selection</title>
      <link>https://jzhao.xyz/thoughts/feature-selection/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/feature-selection/</guid>
      <description>Better features usually help more than a better model. Good features would ideally:
 Allow learning with few examples, hard to overfit with many examples Capture most important aspects of problem Reflects invariances (generalize to new scenarios)  Find the features (columns) of $X$ that are important for predicting $y$</description>
    </item>
    
    <item>
      <title>Gradient descent</title>
      <link>https://jzhao.xyz/thoughts/gradient-descent/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/gradient-descent/</guid>
      <description>When we minimize or maximize a function, we call it optimization.
Gradient descent is essentially an iterative optimization algorithm that takes a guess and refines it using the gradient to make a better guess.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://jzhao.xyz/thoughts/linear-regression/</link>
      <pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/linear-regression/</guid>
      <description>Vector dimensions:
 $w$ is $(d, 1)$ (weights) $y$ is $(n,1)$ (targets) $x_i$ is $(d, 1)$ (features) $X$ is $(n,d)$ each row is $x_i^T$  Linear regression makes predictions $\hat y_i$ using a linear function of $x_i$: $\hat y_i = w^Tx_i$</description>
    </item>
    
    <item>
      <title>Outlier detection</title>
      <link>https://jzhao.xyz/thoughts/outlier-detection/</link>
      <pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/outlier-detection/</guid>
      <description>Find observations that are unusually different from the others (aka anomaly detection).
Why? We may want to remove outliers, or be interested in the outliers themselves (security)</description>
    </item>
    
    <item>
      <title>Density-based clustering</title>
      <link>https://jzhao.xyz/thoughts/density-based-clustering/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/density-based-clustering/</guid>
      <description>Clusters are defined by “dense” regions. Examples in non-dense regions don’t get clustered Clusters can be non-convex  It is non-parametric (there is no fixed number of clusters $k$)</description>
    </item>
    
    <item>
      <title>Hierarchical Clustering</title>
      <link>https://jzhao.xyz/thoughts/hierarchical-clustering/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/hierarchical-clustering/</guid>
      <description>Hierarchical clustering produces a tree of clusterings
 Each node in the tree splits the data into 2 or more clusters.</description>
    </item>
    
    <item>
      <title>K-means</title>
      <link>https://jzhao.xyz/thoughts/K-means/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/K-means/</guid>
      <description>Assumption that we know how many clusters there are as a prior ($k$ in K-Means). Designed for vector [[thoughts/quantization|quantization]]: replacing examples with the mean of their cluster (collapsing a bunch of examples of a class down to a single example)</description>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>https://jzhao.xyz/thoughts/clustering/</link>
      <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/clustering/</guid>
      <description>Set of techniques to find components that belong together.
Note: Grouping is how the human visual system perceives things and clustering is the actual algorithm itself.</description>
    </item>
    
    <item>
      <title>Unsupervised learning</title>
      <link>https://jzhao.xyz/thoughts/unsupervised-learning/</link>
      <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/unsupervised-learning/</guid>
      <description>In [[thoughts/supervised learning|supervised learning]], we have features $x_i$ and class labels $y_i$. Write a program that produces $y_i$ form $x_i$</description>
    </item>
    
    <item>
      <title>Curse of Dimensionality</title>
      <link>https://jzhao.xyz/thoughts/Curse-of-Dimensionality/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Curse-of-Dimensionality/</guid>
      <description>Volume grows exponentially with dimension. Our nearest neighbour in high-dimensions might be really really far away
If want every location on to have a “neighbor” with distance $\epsilon$,</description>
    </item>
    
    <item>
      <title>Ensemble method</title>
      <link>https://jzhao.xyz/thoughts/Ensemble-method/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Ensemble-method/</guid>
      <description>Ensemble methods are classifiers that have classifiers as input (and often have higher accuracy than regular input classifiers). This is also called “meta-learning” and it only works if the individual classifiers make independent errors</description>
    </item>
    
    <item>
      <title>k-Nearest Neighbours (KNN)</title>
      <link>https://jzhao.xyz/thoughts/KNN/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/KNN/</guid>
      <description>To classify an example, we find the $k$ examples closest to the example and take the mode of the $k$ examples.</description>
    </item>
    
    <item>
      <title>Random Forest</title>
      <link>https://jzhao.xyz/thoughts/Random-Forest/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Random-Forest/</guid>
      <description>Example of an [[thoughts/Ensemble method|Ensemble method]]. They are non-parametric
They work by taking a vote from a set of deep [[thoughts/decision tree|decision trees]].</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://jzhao.xyz/thoughts/Naive-Bayes/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Naive-Bayes/</guid>
      <description>An example of a [[thoughts/probability|probabilistic]] classifier. Commonly used in spam filters (classifies as spam if the probability of spam is higher than not spam)</description>
    </item>
    
    <item>
      <title>No Free Lunch Theorem</title>
      <link>https://jzhao.xyz/thoughts/No-Free-Lunch-Theorem/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/No-Free-Lunch-Theorem/</guid>
      <description>All optimization algorithms perform equally well when their performance is averaged across all possible problems.
 There is no &amp;ldquo;best&amp;rdquo; machine learning model.</description>
    </item>
    
    <item>
      <title>Decision Tree</title>
      <link>https://jzhao.xyz/thoughts/decision-tree/</link>
      <pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/decision-tree/</guid>
      <description>A simple program consisting of if-else decisions (decision stumps) based on the features.
 We can create a bunch of decision stumps and define a &amp;ldquo;score&amp;rdquo; for each possible rule.</description>
    </item>
    
    <item>
      <title>Linear Algebra</title>
      <link>https://jzhao.xyz/thoughts/linear-algebra/</link>
      <pubDate>Sun, 11 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/linear-algebra/</guid>
      <description>A lot of content summarized from Mark Schmidt&amp;rsquo;s notes on Linear Algebra
Notation Generally column major
 Scalar (1,1): $\alpha$ Column Vector (m, 1): $\begin{bmatrix}a_1 \ a_2 \end{bmatrix}$ Row Vector (1, n): $\begin{bmatrix}a_1 &amp;amp; a_2\end{bmatrix}$ Matrix (m, n): $\begin{bmatrix}a_{1,1} &amp;amp; a_{2,1} \ a_{1,2} &amp;amp; a_{2,2}\end{bmatrix}$  Operations Transpose $(A^T){ij} = (A){ji}$</description>
    </item>
    
    <item>
      <title>Exploratory data analysis (EDA)</title>
      <link>https://jzhao.xyz/thoughts/exploratory-data-analysis/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/exploratory-data-analysis/</guid>
      <description>How do we &amp;ldquo;look&amp;rdquo; at features and high-dimensional examples?
 Summary statistics  Categorical Features  Frequencies Mode Quantiles   Numerical Features  Location  Mean Median Quantiles   Spread  Range Variance Interquartile ranges     Entropy: measured &amp;ldquo;randomness&amp;rdquo; of a set of variables where entropy is $- \Sigma_{c=1}^k p_c \log p_c$ and $p_c$ is the proportion of times you have value $c$, range from $[0, \log k]$  Low entropy means it is very predictable whereas high entropy means it is very unpredictable (roughly, spread) Normal distribution has the highest entropy    Not always representative!</description>
    </item>
    
    <item>
      <title>Supervised learning</title>
      <link>https://jzhao.xyz/thoughts/supervised-learning/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/supervised-learning/</guid>
      <description>Input: take features of examples and corresponding labels as inputs Output: a model that can accurately predict the labels of new examples  Generally, the most successful machine learning technique (with the exception of games)</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>https://jzhao.xyz/thoughts/machine-learning/</link>
      <pubDate>Fri, 02 Jul 2021 14:03:17 -0400</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/machine-learning/</guid>
      <description>Theory We produce a lot of data (see: [[thoughts/Data Capitalism|data capitalism]])
 [[thoughts/data mining|Data mining]]: automatically extract useful knowledge from large datasets Machine learning: automatically detect patterns in data and use these to make predictions or decisions  Typically, AI $\subset$ ML $\subset$ Deep Learning   Typically, data mining is more human-in-the-loop and more application specific whereas machine learning is more hands-off and general Both similar to statistics but more emphasis on larger datasets, predictions instead of descriptions, and more general models  Healthy skepticism is good though:</description>
    </item>
    
  </channel>
</rss>
