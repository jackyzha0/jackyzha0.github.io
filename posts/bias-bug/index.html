<!DOCTYPE html>
<html lang="en">



<head>
  
  <meta charset="UTF-8" />
  <meta
    name="description"
    content="This blog post was originally intended to be a TED Talk given at TEDxRedmond but I unfortunately was never accepted. However, I did learn a lot in the process of writing it, and I hope you get some value out of it even its presented through just text."
  />
  <title>
    On AI&#39;s &#39;Bias Bug&#39;
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  
  <meta property="og:url" content="https://jzhao.xyz" />
  <meta property="og:title" content="" />
  <meta property="og:description" content="" />
  <meta property="og:image" content="https://jzhao.xyz/res/og-card.png" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="@_jzhao">
  <meta name="twitter:title" content="">
  <meta name="twitter:description" content="" />
  <meta name="twitter:image" content="https://jzhao.xyz/res/og-card.png" />


  
  
  
  
  
  <link rel="shortcut icon" type="image/png"  href="https://jzhao.xyz//icon.png" />
  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  <link href="https://jzhao.xyz/styles.45e048fafd0b345cd4760fbcfdf81beb.min.css" rel="stylesheet" />

  
  <link href="https://jzhao.xyz/styles/_light_syntax.32359fa0e4ad5c5b354cb209e7fa1b22.min.css" rel="stylesheet" id="theme-link">

   
  
  
  
  
  <script src="https://jzhao.xyz/js/darkmode.182a2b4c9451f6f751c276a71c985624.min.js"></script>
  
  
  
  <script src="https://jzhao.xyz/js/util.ba89ec55faeebabb6b4bf288cd40f6da.min.js"></script>
  
  
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" as="style"
      onload="this.onload=null;this.rel='stylesheet'"
      integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
        integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
        crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
        integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
        crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js"
        integrity="sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A"
        crossorigin="anonymous"></script>


  
  



  <script async src="https://unpkg.com/@floating-ui/core@0.7.3"></script>
  <script async src="https://unpkg.com/@floating-ui/dom@0.5.4"></script>
  
  <script async src="https://jzhao.xyz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js"></script>

  
  
  
  <script defer src="https://jzhao.xyz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js"></script>
  

  
  
  <script defer src="https://jzhao.xyz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js"></script>
  

  

  
   
  <script>
    
    const isReducedMotion = window.matchMedia('(prefers-reduced-motion: reduce)').matches
    const lastVisit = localStorage.getItem('lastVisitTime')
    const now = Date.now()
    let show = 'true'
    if (lastVisit) {
      document.documentElement.setAttribute('visited', 'true')
      const minElapsed = Math.ceil((now - parseInt(lastVisit)) / (1000 * 60))
      show = (!isReducedMotion && minElapsed > 5) ? 'true' : 'false'
    }
    document.documentElement.setAttribute('show-animation', show)
    localStorage.setItem('lastVisitTime', `${now}`)

    const SEARCH_ENABLED =  true 
    const LATEX_ENABLED =  true 
    const PRODUCTION =  true 
    const BASE_URL = "https://jzhao.xyz/"
    const fetchData = Promise.all([
          fetch("https:\/\/jzhao.xyz\/indices\/linkIndex.da3e80384562de8f06a4c69058b9e827.min.json")
            .then(data => data.json())
            .then(data => ({
              index: data.index,
              links: data.links,
            })),
          fetch("https:\/\/jzhao.xyz\/indices\/contentIndex.2a8f5f0fb6d625d6685f97d461e0341a.min.json")
            .then(data => data.json()),
        ])
        .then(([{index, links}, content]) => ({
          index,
          links,
          content,
        }))

      const render = () => {
      

      const siteBaseURL = new URL(BASE_URL);
      const pathBase = siteBaseURL.pathname;
      const pathWindow = window.location.pathname;
      const isHome = pathBase == pathWindow;

      addCopyButtons();
      

      addTitleToCodeBlocks();
      

      

      
      initPopover(
        "https://jzhao.xyz",
         true 

      )
      

      
      const footer = document.getElementById("footer")
      if (footer) {
        const container = document.getElementById("graph-container")
        
        if (!container) return requestAnimationFrame(render)
        
        container.textContent = ""

        const drawGlobal = isHome &&  false ;
        drawGraph(
            "https://jzhao.xyz",
            drawGlobal,
            [{"/moc":"#4388cc"}],
            drawGlobal ? {"centerForce":1,"depth":-1,"enableDrag":true,"enableLegend":false,"enableZoom":true,"fontSize":0.5,"linkDistance":1,"opacityScale":3,"repelForce":1,"scale":1.4} : {"centerForce":1,"depth":1,"enableDrag":true,"enableLegend":false,"enableZoom":true,"fontSize":0.6,"linkDistance":0.8,"opacityScale":3,"repelForce":2,"scale":1}
          );

        }
      

      
        var els = document.getElementsByClassName("mermaid");
        if (els.length > 0) {
          import('https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs').then(
            (obj) => {
              
              
              obj.default.init();
            }
          )
        }
      
      
      
      function clickHandler(evt) {
        const target = evt.target 
        const classNames = target.className.split(" ")
        const broken = classNames.includes("broken")
        const internal = classNames.includes("internal-link")
        plausible("Link Click", {
          props: {
            href: target.href,
            broken,
            internal,
            graph: false,
          }
        })
      }

      const links = document.querySelectorAll("a")
      for (link of links) {
        if (link.className.includes("root-title")) {
          link.addEventListener('click', clickHandler, {once: true})
        }
      }
    }

    const init = (doc = document) => {
      
      addCopyButtons();
      

      addTitleToCodeBlocks();
      renderMathInElement(doc.body, {
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false},
        ],
        macros: {
          '’': "'"
        },
        throwOnError : false
      });
      
    };
  </script>
  
  
  <script type="module">
    import { attachSPARouting } from "https:\/\/jzhao.xyz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script>
  
  
  <script defer data-domain="jzhao.xyz" src="https://plausible.io/js/script.js"></script>
  <script>window.plausible = window.plausible || function() { (window.plausible.q = window.plausible.q || []).push(arguments) }</script>
</head>


<body>
<div id="search-container">
  <div id="search-space">
    <input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search"
      placeholder="Search for something..." dir="">
    <div id="results-container">
    </div>
  </div>
</div>


<script defer type="module" src="https://jzhao.xyz/js/semantic-search.928ff7841d5bd97b1043546587762cf5.min.js"></script>



<div id="cursor-chat-layer">
  <input type="text" id="cursor-chat-box">
</div>
<script type="module">
  import { initCursorChat } from 'https://esm.sh/cursor-chat'
  initCursorChat("jzhao.xyz")
</script>

<div class="singlePage">
    
    <header class="delay t-3">
    <h1 id="page-title"><a class="root-title" href="https://jzhao.xyz/">jzhao.xyz</a></h1>
    <div class="spacer"></div>
    <div id="search-icon">
      <p>Search</p>
      <svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg>
    </div>
    <div class='darkmode'>
    <input class='toggle' id='darkmode-toggle' type='checkbox' tabindex="-1">
    <label id="toggle-label-light" for='darkmode-toggle' tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35;" xml:space="preserve">
            <title>Light Mode</title>
            <path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z" />
        </svg>
    </label>
    <label id="toggle-label-dark" for='darkmode-toggle' tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'" xml:space="preserve">
            <title>Dark Mode</title>
            <path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z" />
        </svg>
    </label>
</div>

</header>


    <article>
      <h1>On AI&#39;s &#39;Bias Bug&#39;</h1>
      <p class="meta">
        Last updated 
Aug 29, 2020

 
          
<a href="https://github.com/jackyzha0/quartz/tree/hugo/content/posts/bias-bug.md" rel="noopener">Edit Source</a>


      </p>
      <ul class="tags">
    
    <li><a href="https://jzhao.xyz/tags/fruit/">Fruit</a></li>
    
</ul>

      


      

























<p>This blog post was originally intended to be a TED Talk given at TEDxRedmond but I unfortunately was never accepted. However, I did learn a lot in the process of writing it, and I hope you get some value out of it even its presented through just text.</p>
<hr>
<a href="#hey-google"><h2 id="hey-google"><span class="hanchor" ariaLabel="Anchor"># </span>Hey Google</h2></a>
<p>Take a picture in 5 seconds. Say cheese!</p>
<p>See that? This is just one of many examples of where AI is becoming a larger part of what we accept as normal. What seemed far-fetched and straight out of science fiction a few years ago is now becoming a reality.</p>
<p>Today, tech giants deploy AI to dictate what we see, hear, buy – even feel and think. They control what kind of news we see every morning, suggest places to go, and even drive our cars. In order for us to have a healthy relationship with this technology, we have to adapt as quickly as it is advancing. In doing so, we need to ask ourselves: what kind of decisions AI should be allowed to make?</p>
<a href="#what-kind-of-decisions-ai-should-be-allowed-to-make"><h3 id="what-kind-of-decisions-ai-should-be-allowed-to-make"><span class="hanchor" ariaLabel="Anchor"># </span>What kind of decisions AI should be allowed to make?</h3></a>
<p>Sure, most of you in the audience may be comfortable letting an AI decide what kind of music to add to your Spotify playlist, but when an AI needs to decide what kind of jail time a criminal should face we all get a little squeamish.</p>
<p>In fact, let me give you a few examples of decisions that AI make every single day and see whether you would be comfortable with an AI making that same decision.</p>
<p>

<img src="https://3.bp.blogspot.com/-BCz28oA5THs/W7_1x671dkI/AAAAAAAADXg/h6OcOZnRZl0vN8mw02gNVcwglJTqE87bwCLcBGAs/s1600/image1.png" width="auto" alt="An AI-based breast cancer detector in action"  /><em>Google AI Blog, 2018</em>
Would you be comfortable with an AI helping doctors to identify cancerous tumours?</p>
<p>


<img src="https://jzhao.xyz//posts/images/bias-bug/hiring.png" width="auto" alt="A game of Survival of the Best Fit"  /><em>survivalofthebestfit.com</em>
How about an AI helping companies decide who to hire based on only your name, age, gender, and resume?</p>
<p>

<img src="https://media4.s-nbcnews.com/j/newscms/2016_10/1008761/chihuahua-muffin-today-160311-tease-02_15277fe1b7a8c911f94700f866189fc2.social_share_1024x512_center.jpg" width="auto" alt="Chihuahua or muffin?"  /><em>@teenybiscuit</em>
The classic image detection example: are these chihuahuas or muffins?</p>
<p>

<img src="https://miro.medium.com/max/1400/1*dCk2Jns5Deg4Y2O6KBcuzA.jpeg" width="auto" alt="A modified version of the trolley problem"  /><em>Illustration: Simon Landrein</em>
How about the trolley problem? Should a self-driving car — given no other option — kill A) the child or B) the elderly person?</p>
<p>As you can see, there is a really obvious difference each of those decisions that were proposed. In the tumour example and chihuahua example, you likely weren&rsquo;t super bothered if an AI were to make that decision. Yet for the hiring example and the self-driving car example, you likely were more uncertain.</p>
<a href="#why-the-uncertainty"><h3 id="why-the-uncertainty"><span class="hanchor" ariaLabel="Anchor"># </span>Why the uncertainty?</h3></a>
<p>What we can see from this is that there is a difference between objective and subjective problems. In the tumour and chihuahua examples, we were mostly comfortable with that decision being made because there is a clear ‘right’ or ‘wrong.’ However, in the case of the hiring example and self-driving car example, the subjectivity makes it difficult. A lot of it has to do with what kind of environment we were raised in and how each of us sees the world. Everyone is born with some kind of bias, favouring certain ways of viewing the world.</p>
<p>AI can do a lot of really great things such as helping doctors identify tumours or interpret the world for the deaf. But when used improperly, that subjectivity can propel some of the worst biases we have as humans.</p>
<a href="#garbage-in-garbage-out"><h2 id="garbage-in-garbage-out"><span class="hanchor" ariaLabel="Anchor"># </span>Garbage in, garbage out</h2></a>
<p>There’s a timeless saying “garbage in, garbage out” in the field of Computer Science which essentially states that bad data or bad input will produce an output that’s of equal quality. This holds true for almost all the tech we use today, from trading algorithms to search results. If what we put into the system is inherently unclear or flawed, then the output will also give back something that’s ‘wrong’ or doesn’t align with our objectives.</p>
<p>However, this saying “garbage in, garbage out” is most prevalent in the AI which sits at the forefront of this tech revolution. AI, in one form or another, is still created by humans, who are imperfect, make mistakes, and are inherently biased.</p>
<p>Interestingly, there are two distinct ways that this bias can shine through.</p>
<a href="#1-problem-definition"><h3 id="1-problem-definition"><span class="hanchor" ariaLabel="Anchor"># </span>1. Problem definition</h3></a>
<p>The first is in the problem definition. When creating an AI, we need to define an objective for it. That means putting something vague like “create a realistic human-sounding voice” or “help me translate this speech to French” into definitive, and certain terms and mathematical concepts. How do we do that? Because we don’t have algorithms that do this step for us, this is usually done by a team of machine learning engineers. They are responsible for deciding how to represent our ‘objective’ in terms of penalties and rewards. This also means that how the team of engineers decide to represent the problem is a product of their biases.</p>
<p>Take Amazon for example. In 2014, Amazon decided to create a recruitment engine that was able to look at a job applicant and rate them from a one-star rating to a five-star rating. However, by 2015, Amazon realized that their software was not evaluating candidates for positions related to tech in a gender-neutral way<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Although unintentionally, Amazon’s engineers included a gender field. The algorithm, after sifting through 10 years worth of resumes, began to favour men and penalize women. After deeper inspection, this was most likely an unfortunate reflection of the male-dominated tech industry. The lesson is clear. The algorithm served to reflect this bias that was observed in the past.</p>
<a href="#2-lack-of-data-diversity"><h3 id="2-lack-of-data-diversity"><span class="hanchor" ariaLabel="Anchor"># </span>2. Lack of Data Diversity</h3></a>
<p>The second, less obvious way bias can poison AI is with 





<a
  href="/thoughts/data-distributions/"
  rel="noopener" class="internal-link"
  data-src="/thoughts/data-distributions/">data diversity</a> — or rather the lack of it. I think the best way to explain this is through a metaphor. Imagine the AI as a small child. It likes to learn from its environment. If this child were to be raised in a racist family, it will almost undoubtedly hold similar views in the future. This is a very similar case for AI. It learns from the environment and data it&rsquo;s given.</p>
<p>One case of this is the very first iteration of Google Photo’s image classification feature back in 2015. This feature claimed to be able to identify people, places, and things with high accuracy. Twitter user 
<a href="https://twitter.com/jackyalcine/" rel="noopener">@jackyalcine</a> found that the algorithm identified people with darker skin as gorillas<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Google quickly was able to work and manually ‘patch’ the issue, but the actual issue was much deeper – and it had to do with the data used. In this case, the dataset that Google used to train their algorithm had an over proportional amount of middle-aged Caucasian people under the category of ‘people.’ This meant that while the recognition accuracy was really high for that select group of people, the accuracy for people of colour was significantly worse.</p>
<p>From both cases, we can see that an over-focus on results and accuracy can cause these companies to ignore these 





<a
  href="/thoughts/bias/"
  rel="noopener" class="internal-link"
  data-src="/thoughts/bias/">biases</a>. When the deadline is too tight or the manager sets an expectation for a “10% increase in accuracy,” there is a very strong incentive to ignore the ‘edge cases’ or things that happen very rarely.</p>
<p>Too often our society is focused on the raw accuracy that we forget that the same accuracy metric is something that we set for ourselves – created arbitrarily by humans which have bias. Unfortunately, this results in things like the Google Photos and Amazon Hiring cases.</p>
<a href="#fairness-in-ai"><h2 id="fairness-in-ai"><span class="hanchor" ariaLabel="Anchor"># </span>Fairness in AI</h2></a>
<p>Truth is, data lacks 





<a
  href="/thoughts/context/"
  rel="noopener" class="internal-link"
  data-src="/thoughts/context/">context</a>. While the trends in the data may show that in the past there have been more men in the women in the workforce, the majority of the population can agree that we are moving away from that more traditional view into more of an equal playing ground. Unfortunately, these models that we create don’t have a deeper understanding of these changes and as a result, produces naïve predictions that we believe are wrong or ‘garbage.’ But is it really? Is it really only a ‘bad’ result because of what we define as fair or right?</p>
<p>

<img src="https://static.propublica.org/projects/algorithmic-bias/assets/img/generated/methodology-risk-of-recidivism-scores-by-race-900*363-482d1c.png" width="auto" alt="COMPAS exhibiting bias against those of African descent"  /><em>ProPublica, 2016</em></p>
<p>We can take a look at the COMPAS system which is a piece of software used by U.S. courts to assess the likelihood of a criminal to reoffend. ProPublica, a non-profit newsroom, did an investigation back in 2016<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> and claimed that COMPAS was biased against those of African descent — citing that it overestimated the false positive rate of reoffending for those of African descent by almost twice as high as those for Caucasians. ProPublica reasoned that a fair algorithm would not have such a big difference.</p>
<a href="#so-is-compas-fair"><h3 id="so-is-compas-fair"><span class="hanchor" ariaLabel="Anchor"># </span>So, is COMPAS fair?</h3></a>
<p>Well, there’s no concrete answer.  The algorithm never had any access to any contextual information about the neighbourhoods or the actual situation for each of the offenses. Was the area more heavily policed because it was a predominantly black neighbourhood? Were the officers themselves biased in making arrests?</p>
<p>Even more interestingly, a study done at Dartmouth<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> showed that random volunteers, when given the same information as the COMPAS algorithm, achieved a nearly identical accuracy of identifying the rate of recidivism.</p>
<p>This is interesting. This means that either COMPAS is accurate or holds the exact same biases as we do as a society. Unfortunately, this is a problem that I don’t think we can solve, so we don’t have a solid definition as to what makes an algorithm fair. But what is clear, is that there is bias in play here, whether that be through the police, the companies, the actual algorithm, or society itself. This bias is what causes that “subjectivity” and “garbage in.” This is what is preventing us from making ‘fairer’ AI and applying AI to more tasks.</p>
<a href="#the-inevitability-of-bias"><h3 id="the-inevitability-of-bias"><span class="hanchor" ariaLabel="Anchor"># </span>The inevitability of bias</h3></a>
<p>The point is, unless we work to prevent, catch, and deter bias, it will inevitably occur. One of the biggest problems in the field of AI is that so many of the models exist in a black box, meaning that its inner workings are only known by a select few. This makes it near impossible to identify and train out bias. Machine intelligence will become almost integral to our lives, becoming less visible in the process, and AI’s bias bug will get harder to beat. Our time to act is now.</p>
<a href="#what-we-can-do"><h2 id="what-we-can-do"><span class="hanchor" ariaLabel="Anchor"># </span>What we can do</h2></a>
<blockquote>
<p>So? What can we, as the next generation, do to help?</p>
</blockquote>
<p>First, we need to build a 





<a
  href="/thoughts/explainability/"
  rel="noopener" class="internal-link"
  data-src="/thoughts/explainability/">better understanding</a> of how the AI systems we are building work. Through this understanding, we can better 





<a
  href="/thoughts/trust/"
  rel="noopener" class="internal-link"
  data-src="/thoughts/trust/">trust</a> and, as a result, effectively manage the emerging generation of AI. So, don’t be afraid to learn that programming language you heard about. Read up on how that cool translation algorithm works.</p>
<p>Secondly, we need to diversify. Diversify not only in the sense of having better representation in datasets, but in tech. Next time you pitch a new product, or create a new project, ask yourself this:</p>
<blockquote>
<p>“How many people have you considered before you make that decision?”</p>
</blockquote>
<p>Let&rsquo;s 





<a
  href="/thoughts/Design-Justice/"
  rel="noopener" class="internal-link"
  data-src="/thoughts/Design-Justice/">design for everyone</a>. Consider people of different ethnic groups, sexualities, income, just to name a few. By having more representation in these teams, AI can cater to more than just that select group of people who are western, educated, and rich. Instead, by bringing in a fresh perspective on the problems that these teams are trying to tackle, they can create innovation that benefits a whole range of communities.</p>
<p>With increased diversity and representation, the Google Photo misidentification problem never would have happened. Together, we can help to build a future where diversity is no longer an issue in both machine learning models and tech, but society too. Where we don’t just use AI mindlessly but understand it and use it in such a way that it helps to empower humanity. Where we can work towards a future where we can begin to trust the more subjective decisions that an AI can make.</p>
<p>Together, we can make that future a reality tomorrow. Thanks for coming to my TED talk (unironically this time).</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>
<a href="https://www.businessinsider.com/amazon-built-ai-to-hire-people-discriminated-against-women-2018-10" rel="noopener">News coverage on Amazon Hiring case</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>
<a href="https://www.forbes.com/sites/mzhang/2015/07/01/google-photos-tags-two-african-americans-as-gorillas-through-facial-recognition-software/#2842682a713d" rel="noopener">News coverage on the Google Photos case</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>
<a href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm" rel="noopener">ProPublica Analysis</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>
<a href="https://advances.sciencemag.org/content/4/1/eaao5580" rel="noopener">Dartmouth Study on COMPAS accuracy</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>


    </article>
    <hr/>


<div class="page-end" id="footer">
    <div class="backlinks-container">
        <h3>Backlinks</h3>
<ul class="backlinks">
    
    
    
    
    
    
    
    
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      <li>
        <a href="/thoughts/A-City-is-not-a-Computer/" data-ctx="&#39;imperfect&#39; algorithms" data-src="/thoughts/A-City-is-not-a-Computer" class="internal-link">A City is not a Computer</a>
      </li>
      
      
      
      <li>
        <a href="/thoughts/algorithmic-decision-making/" data-ctx="bias-bug" data-src="/thoughts/algorithmic-decision-making" class="internal-link">Algorithmic decision making</a>
      </li>
      
      
      
      <li>
        <a href="/thoughts/bias/" data-ctx="Bias in AI" data-src="/thoughts/bias" class="internal-link">Bias</a>
      </li>
      
      
      
      <li>
        <a href="/thoughts/semantics/" data-ctx="bias bug" data-src="/thoughts/semantics" class="internal-link">Semantics</a>
      </li>
      
      
      
</ul>

    </div>
    <div>
        <script
  async
  src="https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js"
  integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI="
  crossorigin="anonymous"
></script>
<h3>Interactive Graph</h3>
<div id="graph-container"></div>
<style>
  :root {
    --g-node: var(--secondary);
    --g-node-active: var(--primary);
    --g-node-inactive: var(--visited);
    --g-link: var(--outlinegray);
    --g-link-active: #5a7282;
  }
</style>

<script src="https://jzhao.xyz/js/graph.6579af7b10c818dbd2ca038702db0224.js"></script>

    </div>
</div>






<div id="contact_buttons">
    <footer>
        
        
        <p>Made by Jacky Zhao using <a href="https://github.com/jackyzha0/quartz">Quartz</a>, © 2023</p>
        <ul>
            
            <li><a href="https://jzhao.xyz/">Home</a></li>
            <li><a href="https://twitter.com/_jzhao">Twitter</a></li><li><a href="https://github.com/jackyzha0">GitHub</a></li></ul>
    </footer>
</div>


</div>
</body>
</html>
